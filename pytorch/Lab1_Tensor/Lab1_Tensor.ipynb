{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883caaba-ae82-4e1d-94f4-81f3c9813d08",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1954f015-64ce-4fd5-9102-884368ac51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21294d-6659-4c83-aba0-6e81e6a0c064",
   "metadata": {},
   "source": [
    "## Initializing a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a222a2-27a1-450c-845b-47860c8c5b8e",
   "metadata": {},
   "source": [
    "#### Directly from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5a6b791-c0c8-4ca8-8cda-350e1ac67a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      " [[1, 2], [3, 4]] \n",
      "\n",
      "data.type: \n",
      " <class 'list'> \n",
      "\n",
      "x_data: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "x_data.type: \n",
      " <class 'torch.Tensor'> \n",
      "\n",
      "x_data.shape: \n",
      " torch.Size([2, 2]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[1,2], [3,4]]\n",
    "print(f\"data: \\n {data} \\n\") \n",
    "print(f\"data.type: \\n {type(data)} \\n\") # <class 'list'> \n",
    "#print(f\"data.shape: \\n {data.shape} \\n\") # Error\n",
    "\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"x_data: \\n {x_data} \\n\")\n",
    "print(f\"x_data.type: \\n {type(x_data)} \\n\") # <class 'torch.Tensor'> \n",
    "print(f\"x_data.shape: \\n {x_data.shape} \\n\") # torch.Size([2, 2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ecc1b-5308-4686-ba8c-1deabababe26",
   "metadata": {},
   "source": [
    "#### From a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "390bde65-0965-4bd9-8c8f-cd5f9c186235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_array: \n",
      " [[1 2]\n",
      " [3 4]] \n",
      "\n",
      "x_np: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"np_array: \\n {np_array} \\n\")\n",
    "print(f\"x_np: \\n {x_np} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956db6b3-3f63-478c-a3b9-5f85f08612bb",
   "metadata": {},
   "source": [
    "#### From another tensor:\n",
    "\n",
    "명시적으로 재정의(Override)하지 않는다면, 인자로 주어진 Tensor의 속성(shape-모양, datatype-자료형)을 유지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3211fd3-a335-41cb-ac8a-3b6ae7066da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.0609, 0.3342],\n",
      "        [0.6047, 0.9495]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # x_data의 속성을 유지\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의  속성을 덮어쓴다.\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f7611-1f00-409c-a3ce-e579c95d4ee2",
   "metadata": {},
   "source": [
    "#### With random or constant values:\n",
    "\n",
    "- shape은 Tensor의 차원(dimension)을 나타내는 튜플(tuple) 형태이다.\n",
    "- 아래 함수들에서는 출력 텐서의 차원을 결정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aa150b8-69a4-4306-8597-af46edf2a853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.8438, 0.3692, 0.2099],\n",
      "        [0.2872, 0.2071, 0.6097]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd713dc-abc4-49f3-8113-391245fff73f",
   "metadata": {},
   "source": [
    "## Attributes of a Tensor\n",
    "\n",
    "\n",
    "- Tensor 의 속성은 shape, datatype 및 어느 장치에 저장되는지를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfc61771-6f7d-431e-8269-61a807ab1192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4]) \n",
      "Datatype of tensor: torch.float32 \n",
      "Device tensor is stored on: cpu \n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape} \")\n",
    "print(f\"Datatype of tensor: {tensor.dtype} \")\n",
    "print(f\"Device tensor is stored on: {tensor.device} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73084cf8-f567-4d1d-8828-d3f0408ba568",
   "metadata": {},
   "source": [
    "## Operations on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f3316-0b89-4a54-9b86-cbda1c121f00",
   "metadata": {},
   "source": [
    "- 전치(transposing, 인덱싱(indexing), 슬라이싱(slicing), 수학 계산, 선형 대수, 임의 샘플링(random sampling) 등 100가지 이상의 텐선 연산이 가능\n",
    "- 각 연산들은 (일반적으로 CPU보다 빠른) GPU 에서 실행할 수 있다.\n",
    "- Colab 을 사용한다면 Edit > Notebook Settings 에서 GPU를 할당할 수 있다.\n",
    "- 기본적으로 Tensor는 CPU에 생성된다.\n",
    "- GPU의 가용성(availability)을 확인한 뒤 .to 메소드를 사용하면 GPU로 텐서를 명시적으로 이동할 수 있다.\n",
    "- 장치들 간에 큰 텐서들을 복사하는 것은 시간과 메모리 측면에서 비용이 많이 든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a32c81ac-e48d-4673-88c1-cb7b8ee8786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU가 존재하면 Tensor를 이동한다.\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554204b-a8e0-4b79-aedf-3f0408bb24e2",
   "metadata": {},
   "source": [
    "몇몇 연산들을 시도 해 본다. NumPy API 에 익숙하다면 Tensor API 를 사용하는 것은 쉽다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c870588e-1f4a-45c9-ab3e-afeac9b891fc",
   "metadata": {},
   "source": [
    "#### Standard numpy-like indexing and slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53442cc2-06ea-495e-b831-d1eacf10db17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4,4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31396c05-855d-4954-8209-7b5d340e42f8",
   "metadata": {},
   "source": [
    "#### Joining tensors\n",
    "\n",
    "- torch.cat 을 사용하여 주어진 차원에 따라 일련의 텐서를 연결할 수 있다.\n",
    "- torch.cat 과 미묘하게 다른 또 다른 텐서 결합 연산인 torch.stack 도 참고할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06f7c04d-9e5d-41c7-b4e4-316f26b1144e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e59e4-72a8-43ad-91ec-d29272be12a4",
   "metadata": {},
   "source": [
    "#### Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd0032c0-f83f-4295-b90e-1485a165f9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두 텐서 간의 행렬 곱(matrix multiplication)을 계산한다.\n",
    "# y1, y2, y3 는 모두 같은 값을 갖는다.\n",
    "# tensor.T 는 텐서의 전치(transpose)를 반환한다.\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "# 요소 별 곱(element-wise product)을 계산한다.\n",
    "# z1, z2, z3는 모두 같은 값을 갖는다.\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3506abe-33e6-476d-825c-7363110c0a73",
   "metadata": {},
   "source": [
    "#### Single-element tensors\n",
    "\n",
    "- 텐서의 모든 값을 하나로 집계(aggregate)하여 요소가 하나인 텐서의 경우, item()을 사용하여 Python 숫자값으로 변환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3110820-1648-446c-9222-f9905a811abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1db33-459f-4d8d-8537-e1bb234aa42f",
   "metadata": {},
   "source": [
    "#### In-place operations\n",
    "\n",
    "- 연산 결과를 피연산자(operand) 에 저장하는 연산을 바꿔치기 연산이라고 한다.\n",
    "- _ 접미사를 갖는다.\n",
    "- 예를 들어, x_copy_(y) 나 x.t_() 는 x f를 변경한다.\n",
    "- 바꿔치기 연산은 메모리를 일부 절약하지만, 기록(history)이 즉시 삭제되어 도함수(derivate) 계산에 문제가 발생할 수 있다. 따라서 사용을 권장하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fc4245e-d612-44c6-b7c1-0e0a814722f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f201eec-ba63-49a4-b8af-75c8df078be4",
   "metadata": {},
   "source": [
    "## Bridge with NumPy\n",
    "\n",
    "- CPU 상의 텐서와 NumPy 배열은 메모리 공간을 공유하기 때문에 하나를 변경하면 다른 하나도 변경된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918416f3-571d-46bc-baa8-2b62bbc42e45",
   "metadata": {},
   "source": [
    "#### Tensor to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ef7a876-ee27-46bc-82e0-6bfafbb7966d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0768838-5c45-4a95-b838-094eac119655",
   "metadata": {},
   "source": [
    "텐서의 변경 사항이 NumPy 배열에 반영된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9975f19f-258d-4fbb-94fe-ae13e92a49b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6d36e-b3ed-418f-95a2-128e0de3895d",
   "metadata": {},
   "source": [
    "#### NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87a332da-32a4-42c4-bcf7-f7d3452f66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c064884-5e22-4058-ba05-5854472ef7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n,1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d400e-ad21-4aea-9743-e7e25350adbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
